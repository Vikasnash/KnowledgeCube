## DeepSeek & LLM Learning Links

A curated list of resources to understand DeepSeek, Attention Mechanisms, Mixture of Experts, and LLM internals.

---

### ðŸ”¹ Foundational Concepts

1. [DeepSeek series introduction](https://lnkd.in/gRcNE-sg) â€“ Overview of the DeepSeek project  
2. [DeepSeek basics](https://lnkd.in/gEUzFtrC) â€“ Foundational concepts behind DeepSeek  
3. [Journey of a token into the LLM architecture](https://lnkd.in/gUnabApX) â€“ Visual walkthrough of token flow inside an LLM  
4. [Attention mechanism explained in 1 hour](https://lnkd.in/gSVDn2e) â€“ Full explanation of attention in LLMs  
5. [Self Attention Mechanism â€“ Handwritten from scratch](https://lnkd.in/gN85qVSK) â€“ Learn attention without relying on libraries  

---

### ðŸ”¹ Attention Mechanisms

6. [Causal Attention Explained](https://lnkd.in/gNPwCJWT) â€“ Why models donâ€™t look into the future  
7. [Multi-Head Attention Visually Explained](https://lnkd.in/gPr-XrwE) â€“ Clear visual breakdown  
8. [Multi-Head Attention Handwritten from Scratch](https://lnkd.in/gzb6Xvy6) â€“ Raw implementation guide  
9. [Key Value Cache from Scratch](https://lnkd.in/gn8zgdc7) â€“ Build a caching mechanism for faster inference  
10. [Multi-Query Attention Explained](https://lnkd.in/gvt3AyRh) â€“ Efficiency tweak in attention computation  
11. [Grouped Query Attention (GQA)](https://lnkd.in/geFFq_95) â€“ Specialized version of MHA with grouped queries  
12. [Multi-Head Latent Attention From Scratch](https://lnkd.in/g3z8PkCZ) â€“ Learn about latent attention techniques  
13. [Latent Attention coded from Scratch in Python](https://lnkd.in/g9drDgGZ) â€“ Code walkthrough  

---

### ðŸ”¹ Positional Encodings

14. [Integer and Binary Positional Encodings](https://lnkd.in/eC3USqBF) â€“ Alternate ways to add position info to tokens  
15. [Sinusoidal Positional Encodings](https://lnkd.in/e_6CAdnk) â€“ Commonly used positional encoding  
16. [Rotary Positional Encodings](https://lnkd.in/emHARqyP) â€“ A more recent alternative  
17. [How DeepSeek implemented MLA + RoPE](https://lnkd.in/esbk5SGR) â€“ DeepSeekâ€™s approach  

---

### ðŸ”¹ Mixture of Experts (MoE)

18. [MoE Introduction](https://lnkd.in/emYCfZuc) â€“ Basic idea behind MoE architecture  
19. [MoE Hands-on Demonstration](https://lnkd.in/eRnRcYxF) â€“ Practical example  
20. [MoE Balancing Techniques](https://lnkd.in/ef4qeqPy) â€“ Managing load across expert models  
21. [How DeepSeek rewrote MoE](https://lnkd.in/gkxGYtPv) â€“ DeepSeekâ€™s MoE optimization  
22. [Code MoE from Scratch in Python](https://lnkd.in/g4ZD4kHz) â€“ DIY coding of MoE  

---

### ðŸ”¹ Multi-Token Prediction

23. [Multi-Token Prediction Introduction](https://lnkd.in/gJeWrVxs) â€“ Predicting multiple tokens  
24. [DeepSeek Multi-Token Prediction](https://lnkd.in/gwEzZb49) â€“ Implementation details  
25. [Multi-Token Prediction Coded from Scratch](https://lnkd.in/g67bebXD) â€“ Code example  

---

### ðŸ”¹ LLM Quantization

26. [Introduction to LLM Quantization](https://lnkd.in/ga9_CGMM) â€“ Making LLMs smaller and faster  
27. [Quantization Part 1 by DeepSeek](https://lnkd.in/gpucbYGa) â€“ First phase of DeepSeekâ€™s quantization  
28. [Quantization Part 2 by DeepSeek](https://lnkd.in/g_PicEtY) â€“ Second phase  

---

### ðŸ”¹ Final Summary

29. [Build DeepSeek from Scratch (20-minute Summary)](https://lnkd.in/gVmh6R94) â€“ Fast-track guide to building the full DeepSeek stack

---